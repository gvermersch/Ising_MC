{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Algorithme de Metropolis-Hasting\n",
    "\n",
    "Soit une chaîne de Markov dont la tansition $x \\to y$ a pour probabilité $g(y|x)$ (avec $g$ connue et facilement échantillonable) et soit $x_0$ un point de départ. Pour chaque étape $n$:\n",
    "\n",
    "1. Générer une variable $y \\sim g(.|x_{n})$\n",
    "2. Calculer $r=\\frac{\\pi(\\;y\\,)g(\\;y\\,|x_n)}{\\pi(x_n)g(x_n|\\;y\\,)}$. Générer une variable $u\\sim\\mathscr{U}[0,1]$. Poser $x_{n+1}=y$ si $u \\leqslant r$, ou retourner à (1) sinon.\n",
    "\n",
    "Intuitivement, cette la suite $(x_n)$ converge bien en loi vers $\\pi$ puisqu'on accepte toujours de se déplacer vers les états $y$ plus probables (selon $\\pi$) que l'état actuel $x_n$. Néanmoins, on accepte avec une certaine probabilité $r$ de se déplacer vers un état $y$ moins probable, afin de bien visiter tout l'espace surlequel $\\pi$ est définie.\n",
    "\n",
    "On remarque — c'est important en pratique — que le calcul de $r$ ne nécessite de connaître $\\pi$ qu'**à une constante multiplicative près**, et c'est tout l'intérêt de cette algorithme.\n",
    "\n",
    "Lorsque l'espace surlequel $\\pi$ est défini devient grand, chaque transition $\\mathbf{x} \\to \\mathbf{y}$ (où nous utilisons le gras pour souligner le caractère vectoriel des variable) devient moins probable, de sorte que l'algorithme de Metropolis-Hasting devient lent puisque de très nombreux états $\\mathbf{x}_t$ doivent être traversés afin de converger vers $\\pi$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soit $y_{i}$ le niveau de gris du pixel $i$ et les $x_{i} \\in \\{0,1\\}$, vraie valeur d'un pixel blanc ou noir, suivant une loi d'Ising :  \n",
    "$$ p(x)=\\frac{1}{Z\\left(\\alpha, \\beta \\right)} \\exp \\left[\\alpha \\sum_{i} x_{i}+\\beta \\sum_{i=j} \\mathbb{1}\\left[x_{i}=x_{j}\\right]\\right\\}$$\n",
    "\n",
    "On considère le modèle de la forme : $ y_{i} \\mid x_{i}=k \\sim \\mathcal{N}\\left(\\mu_{k}, \\tau^{2}\\right) $\n",
    "\n",
    "On veut calculer  $\\mathbb{P}(X \\mid Y) = \\frac{\\mathbb{P}(Y \\mid X) \\mathbb{P}(X)}{\\mathbb{P}(Y)} = \\frac{\\mathbb{P}(X \\cap Y)}{\\mathbb{P}(Y)}  $\n",
    "\n",
    "On génère les pixels un par un. Comme les $x_{i}$ ne peuvent prendre que deux valeurs, à savoir 0 ou 1, $X$ suit une loi de Bernouilli dont on cherche à déterminer le paramètre. \n",
    "\n",
    "$ y_{i} \\mid x_{i}=k \\sim \\mathcal{N}\\left(\\mu_{k}, \\tau^{2}\\right) $ donc $\\mathbb{P}(y_{i} \\mid x_{i} = k) = \\frac{1}{\\sqrt{2\\pi \\tau^{2}}} exp \\{\\frac{-1}{2\\tau^{2}} \\left(y_{i} - \\mu_{k}\\right)^{2} \\} $\n",
    "\n",
    "Par la formule des probabilités totales et de Bayes, on a : $\\mathbb{P}(X_{i} =1 \\mid Y_{i}, X_{i}) = \\frac{\\mathbb{P}(X_{i} = 1 \\cap Y_{i}, X_{i})}{\\sum_{x_{i} \\in \\{0,1\\}} \\mathbb{P}(X_{i} = x_{i} \\cap Y_{i}, X_{i})}$\n",
    "\n",
    "$ \\mathbb{P}(X_{i} =1 \\mid Y_{i}, X_{i}) = \\frac{\\frac{1}{Z\\left(\\alpha, \\beta \\right)} \\frac{1}{\\sqrt{2\\pi \\tau^{2}}} exp\\{ \\alpha \\sum_{i} x_{i} + \\beta \\mathbb{1} \\{ x_{j} = 1 \\} - \\frac{1}{2 \\tau^{2}} \\left(y_{i} - \\mu_{1} \\right)^{2} \\}  }{\\sum_{x_{i} \\in \\{0,1\\}} \\frac{1}{Z\\left(\\alpha, \\beta \\right)} \\frac{1}{\\sqrt{2\\pi \\tau^{2}}} exp\\{ \\alpha \\sum_{i} x_{i} + \\beta \\mathbb{1} \\{ x_{j} = 1 \\} - \\frac{1}{2 \\tau^{2}} \\left(y_{i} - \\mu_{k}(x_{i}) \\right)^{2} \\} }  $ \n",
    "\n",
    "\n",
    "En divisant en haut et en bas par le numérateur, on obtient : $ \\mathbb{P}(X_{i} =1 \\mid Y_{i}, X_{i}) = \\frac{1}{1 + exp(N)} $ avec \n",
    "\n",
    "$ N =  -\\alpha + \\beta \\sum_{j} [ \\mathbb{1} \\{ x_{j} = 0 \\} - \\mathbb{1}\\{  x_{j} = 1 \\} ] - \\frac{1}{2\\tau^{2}} \\left(y_{i} - \\mu_{0}\\right)^{2} + \\frac{1}{2\\tau^{2}} \\left(y_{i} - \\mu_{1}\\right)^{2} $\n",
    "\n",
    "$ N = \\frac{1}{2\\tau^{2}} \\left[ \\left(y_{i}^2 - 2\\mu_{1} y_{i} + \\mu_{1}^2) - (y_{i}^2 - 2\\mu_{0} y_{i} + \\mu_{0}^2 \\right) \\right] $ \n",
    "\n",
    "$ N = \\frac{1}{2\\tau^{2}} \\left[ \\mu_{1}^{2} - \\mu_{0}^{2} + 2 y_{i} \\left(\\mu_{0} - \\mu_{1}\\right) \\right] $ \n",
    "\n",
    "Finalement, on obtient : \n",
    "\n",
    "$ \\mathbb{P}(X_{i} = 1 \\mid Y_{i}, X_{i}) = \\frac{1}{1 + exp \\{ -\\alpha + \\beta \\sum_{j} [ \\mathbb{1} \\{ x_{j} = 0 \\} - \\mathbb{1}\\{  x_{j} = 1 \\} ] + \\frac{1}{2\\tau^{2}} \\left[ \\mu_{1}^{2} - \\mu_{0}^{2} + 2 y_{i} \\left(\\mu_{0} - \\mu_{1}\\right) \\right] \\} }$\n",
    "\n",
    "De plus, comme $\\mathbb{P}(X_{i} = 0 \\mid Y_{i}, X_{i}) = 1 - \\mathbb{P}(X_{i} = 1 \\mid Y_{i}, X_{i}) $\n",
    "\n",
    "De la même manière, on obtient :  $ \\mathbb{P}(X_{i}=0 \\mid Y_{i}, X_{i}) = \\frac{1}{1 + exp \\{ \\alpha + \\beta \\sum_{j} [ \\mathbb{1} \\{ x_{j} = 0 \\} - \\mathbb{1}\\{  x_{j} = 1 \\} ] + \\frac{1}{2\\tau^{2}} \\left[ 2y_{i} (\\mu_{1} - \\mu_{0}) + (\\mu_{0}^{2} - \\mu_{1})^{2} \\right]  \\} } $ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut calculer, de manière exacte \n",
    "\n",
    "$P(\\tau^2 \\mid X,Y) = \\frac{P(\\tau^2, X, Y)}{\\int_{0}^\\infty P(\\tau^2, X, Y)d\\tau^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et on a toujours:\n",
    "\n",
    "$\\left.P(X, Y \\mid \\tau^2)=\\frac{1}{Z(\\alpha, B)}\\left(\\frac{1}{\\sqrt{2 \\pi \\tau^{2}}}\\right)^{n} exp( \\alpha \\sum_{i} x_{i}+ beta \\sum_{i\\sim j} \\mathbb{1}\\left(x_{i}=x_{j}\\right)-\\frac{1}{2\\tau^2} \\sum\\left(y_{i}-\\mu_k(i)\\right)^{2}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va utiliser pour $\\tau^2$ un prior de type Inverse-Gamma:\n",
    "\n",
    "$f(\\tau^2 ; a, b)=\\frac{b^{a}}{\\Gamma(a)}(1 / \\tau^2)^{a+1} \\operatorname{exp}(-b / \\tau^2)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puisque beaucoup d'élements ne dépendent pas de $\\tau$, on peut écrire:\n",
    "\n",
    "$P(\\tau^2, X, Y)=P(\\tau^2) \\times P(X, Y \\mid \\tau^2)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\propto \\left(\\tau^{2}\\right)^{-n}\\left(\\tau^{2}\\right)^{-(a+1)} \\times {exp}\\left\\{-\\frac{1}{2 \\tau^{2}} \\sum\\left(y_{i}-\\mu_{k}(i)\\right)^{2}-\\frac{b}{\\tau^{2}}\\right\\}$\n",
    "\n",
    "$\\propto \\left(\\tau^{2}\\right)^{-(a+n+1)} \\exp \\left\\{-\\frac{1}{\\tau^{2}}\\left(\\frac{1}{2} \\sum\\left(y_{i}-\\mu_{k}(i))^{2}+b)\\right\\}\\right.\\right.$\n",
    "\n",
    "\n",
    "\n",
    "On a d'autre part:\n",
    "\n",
    "$\\int_{0}^{+\\infty} x^{-D} \\operatorname{exp}(-E / x)=\\Gamma(D-1) E^{1-D}$\n",
    "Ici:\n",
    "* $x=\\tau^{2}$ \n",
    "* $E=\\frac{1}{2} \\sum\\left(y_{i}-\\mu_{k}(i)\\right)^{2}+b$\n",
    "* $D = a+n+1$\n",
    "\n",
    "Donc\n",
    "$\\int_{0}^{+\\infty} f\\left(\\tau^{2}, X, Y\\right) d \\tau^{2}=\\Gamma(a+n) E^{-(a+n)}$\n",
    "\n",
    "D'où:\n",
    "\n",
    "$\\begin{aligned} P\\left(\\tau^{2}, X, Y\\right) &=\\frac{\\left(\\tau^{2}\\right)^{-(a+n+1)} \\operatorname{exp}\\left(-E / \\tau^{2}\\right)}{\\Gamma(a+n) E^{-(a+n)}} \\\\ &=\\frac{\\left(1 / \\tau^{2}\\right)^{A^{\\prime} +1} E^{A^{\\prime}} \\operatorname{exp}\\left(-E / \\tau^{2}\\right)}{\\Gamma\\left(A^{\\prime}\\right)} \\\\ \\text { Avec } A^{\\prime}=a+n \\end{aligned}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement on voit que si on utilise un prior de la forme:\n",
    "\n",
    "$$\\tau^2 \\sim \\mathcal{IG}(a,b)$$\n",
    "\n",
    "Alors le posterior sera:\n",
    "\n",
    "$$P(\\tau^2 \\mid X,Y) \\sim \\mathcal{IG}(A^{\\prime},E)$$ \n",
    "\n",
    "Avec:\n",
    "\n",
    "* $A^{\\prime} = a + n$\n",
    "* $E = b + \\frac{1}{2} \\sum\\left(y_{i}-\\mu_{k}(i)\\right)^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "\n",
    "* Projets proches du notre\n",
    "    * https://pchanda.github.io/IsingModelDenoising/ \n",
    "    * https://towardsdatascience.com/image-denoising-with-gibbs-sampling-mcmc-concepts-and-code-implementation-11d42a90e153\n",
    "* Page wikipédia du gibbs sampling: https://en.wikipedia.org/wiki/Gibbs_sampling \n",
    "* Un rapport d'un travail de recherche sur la restauration d'images: https://michaelkarpe.github.io/computer-vision-projects/restoration/Report_Markovian_Image_Restoration.pdf\n",
    "* Modèle d'Ising\n",
    "    * https://people.csail.mit.edu/rameshvs/content/ising-gibbs.pdf\n",
    "    * https://pchanda.github.io/Ising-Model/\n",
    "* Paramétrisation correct de la loi inverse-gamma en python: https://distribution-explorer.github.io/continuous/inverse_gamma.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
